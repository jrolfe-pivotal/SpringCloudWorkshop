= Instructions for Pushing Catalog App

In this lab, we will recreate the ElasticSearch bosh release I deployed into your environment.  Note that this is a non-trivial BOSH release, and so is more complex than I might otherwise choose for a lab like this.  

== Prerequisites

https://bosh.io/docs/bosh-cli.html

== Create a new Bosh Release

. Generate the release skeleton
+
----
> bosh init release elasticsearch-boshrelease

Release directory initialized
----
+
. Download the java 8 binary, http://www.linuxfromscratch.org/blfs/view/svn/general/java.html. These instructions assume that binary is named OpenJDK-1.8.0.92-x86_64-bin.tar.xz
. Download the Elasticsearch binary, https://www.elastic.co/downloads/past-releases/elasticsearch-1-7-3 (zip).  These instructions assume that binary is named elasticsearch-1.7.3.zip
. cd into the elasticsearch-boshrelease directory
. Create the subdirectories `blobs/java8` and `blobs/elasticsearch17` and copy the downloaded binaries into these directories (jdk into java8, elasticsearch into elasticsearch17)
. Generate the java8 and elastic search bosh package skeletons
+
----
> bosh generate package java8

create	packages/java8
create	packages/java8/packaging
create	packages/java8/pre_packaging
create	packages/java8/spec

Generated skeleton for `java8' package in `packages/java8'

> bosh generate package elasticsearch17

create	packages/elasticsearch17
create	packages/elasticsearch17/packaging
create	packages/elasticsearch17/pre_packaging
create	packages/elasticsearch17/spec

Generated skeleton for `elasticsearch17' package in `packages/elasticsearch17'
----
+
. Edit the java8 package spec
+
----
---
name: java8

dependencies: []

files:
- java8/OpenJDK-1.8.0.92-x86_64-bin.tar.xz
----
+
. Edit the java8 packaging script
+
----
# abort script on any command that exits with a non zero value
set -e

tar xfv $BOSH_COMPILE_TARGET/java8/OpenJDK-1.8.0.92-x86_64-bin.tar.xz
cp -a OpenJDK-1.8.0.92-x86_64-bin/* $BOSH_INSTALL_TARGET
----
+
. Edit the elasticsearch17 package spec
+
----
---
name: elasticsearch17

dependencies: []

files:
- elasticsearch17/elasticsearch-1.7.3.zip
----
+
. Create the packaging script for elastic search
+
----
# abort script on any command that exits with a non zero value
set -e

unzip $BOSH_COMPILE_TARGET/elasticsearch17/elasticsearch-1.7.3.zip
cp -a elasticsearch-1.7.3/* $BOSH_INSTALL_TARGET
----
+
. From the root directory of the bosh release, generate the elasticsearch job skeleton
+
----
> bosh generate job elasticsearch17
----
+ 
. Create the elasticsearch control script, named `elasticsearch_ctl` in the job's template/bin directory (you'll need to create `bin`)
+
----
#!/bin/bash

set -e # exit immediately if a simple command exits with a non-zero status
set -u # report the usage of uninitialized variables

# Setup env vars and folders for the webapp_ctl script
source /var/vcap/jobs/elasticsearch17/helpers/ctl_setup.sh 'elasticsearch17'

export PORT=${PORT:-5000}
export LANG=en_US.UTF-8

<% p("elasticsearch.exec.environment", {}).each do | k, v | %>
export <%= k %>=<%= v %>
<% end %>

<% if not p('elasticsearch.exec.environment', {}).has_key? 'ES_HEAP_SIZE' then %>
export ES_HEAP_SIZE=$((( $( cat /proc/meminfo | grep MemTotal | awk '{ print $2 }' ) * 46 ) / 100 ))K
<% end %>


PLUGIN_SCRIPTS_DIR=/var/vcap/data/elasticsearch17/plugin-scripts

case $1 in

  start)
    pid_guard $PIDFILE $JOB_NAME
    pid_guard $PIDFILE-init $JOB_NAME-init

    # lock while we update plugins and run drain
    echo "$$" > "$PIDFILE-init"

    mkdir -p /var/vcap/packages/elasticsearch17/plugins

    # install plugins
    rm -rf /var/vcap/packages/elasticsearch17/plugins/*

    # install kopf as it is always packaged with the elasticsearch job
    #/var/vcap/packages/elasticsearch/bin/plugin install file:///var/vcap/packages/elasticsearch-plugins-kopf/elasticsearch-kopf.zip

    # install user defined plugins
    <% p("elasticsearch.plugins").each do |plugin| _, path = plugin.first %>
      <% if path.start_with? '/var/vcap' %>
        /var/vcap/packages/elasticsearch17/bin/plugin install "file://<%= path %>"
      <% else %>
        /var/vcap/packages/elasticsearch17/bin/plugin install "<%= path %>"
      <% end %>
    <% end %>


    ulimit -n 64000
    ulimit -l unlimited  # required to enable elasticsearch's mlockall setting

    mkdir -p $PLUGIN_SCRIPTS_DIR

    # v21 switched to running as vcap; remove after a couple versions
    chown -R vcap:vcap $STORE_DIR $LOG_DIR $RUN_DIR $PLUGIN_SCRIPTS_DIR

    # ES2.0 deprcated -Des.config and insists on having the config file under ES_HOME/config
    cp --remove-destination $JOB_DIR/config/elasticsearch.yml /var/vcap/packages/elasticsearch17/config

    chpst -u vcap:vcap /var/vcap/packages/elasticsearch17/bin/elasticsearch \
         -p ${PIDFILE} \
         --config=/var/vcap/packages/elasticsearch17/config/elasticsearch.yml
         --XX:HeapDumpPath=${TMPDIR}/heap-dump/ \
         <%= p("elasticsearch.exec.options", []).join(' ') %> \
         >>$LOG_DIR/$JOB_NAME.stdout.log \
         2>>$LOG_DIR/$JOB_NAME.stderr.log
    ;;

  stop)
    kill_and_wait $PIDFILE

    ;;
  *)
    echo "Usage: elasticsearch_ctl {start|stop}"

    ;;

esac
exit 0
----
+
. Edit the elasticsearch17 job's monit file
+
----
check process elasticsearch
  with pidfile /var/vcap/sys/run/elasticsearch17/elasticsearch17.pid
  start program "/var/vcap/jobs/elasticsearch17/bin/elasticsearch_ctl start" with timeout 120 seconds
  stop program "/var/vcap/jobs/elasticsearch17/bin/elasticsearch_ctl stop"
  group vcap
----
+
. Edit the elasticsearch17 job's spec file.  Note that this spec file is more complex than might be necessary for a example lab, as I was trying to build a semi-real elasticsearch deployment.  It includes many property definitions to allow for a slightly more sophisticated ability to customize the elasticsearch deployment.
+
----
---
name: elasticsearch17
packages:
- java8
- elasticsearch17
templates:
  bin/elasticsearch_ctl: bin/elasticsearch_ctl
  config/config.yml.erb: config/elasticsearch.yml
  config/logging.yml.erb: config/logging.yml
  config/custom/catalog/synonyms.txt: config/custom/catalog/synonyms.txt
  config/custom/catalog/dimensions.txt: config/custom/catalog/dimensions.txt
  data/properties.sh.erb: data/properties.sh
  helpers/ctl_setup.sh: helpers/ctl_setup.sh
  helpers/ctl_utils.sh: helpers/ctl_utils.sh
properties:
  elasticsearch.drain:
    description: Whether to use the built-in drain features to improve deployment reliability
    # disabled while we do additional testing
    default: false
  elasticsearch.master_hosts:
    description: The list of elasticsearch master node IPs
  elasticsearch.cluster_name:
    description: The name of the elastic search cluster
  elasticsearch.log_level:
    description: The default logging level (e.g. WARN, DEBUG, INFO)
    default: INFO
  elasticsearch.node.allow_master:
    description: Allow node to become master? (true / false)
    default: false
  elasticsearch.node.allow_data:
    description: Allow node to store data? (true / false)
    default: false
  elasticsearch.node.tags:
    description: A hash of additional tags for the node
  elasticsearch.exec.environment:
    description: A hash of additional environment variables for the process
  elasticsearch.exec.options:
    description: An array of additional options to pass when starting elasticsearch
    default: []
  elasticsearch.discovery.minimum_master_nodes:
    description: The minimum number of master eligible nodes a node should "see" in order to operate within the cluster. Recommended to set it to a higher value than 1 when running more than 2 nodes in the cluster.
    default: 1
  elasticsearch.config_options:
    description: "Additional options to append to elasticsearch's config.yml (YAML format)."
    default: ~
  elasticsearch.logging_options:
    description: "Additional options to append to elasticsearch's logging.yml (YAML format)."
    default: ~
  elasticsearch.plugins:
    description: "Plugins to run elasticsearch with (array[] = { plugin-name: install-source }; e.g. [ { kopf: 'lmenezes/elasticsearch-kopf' } ])"
    default: []
  elasticsearch.http_host:
    description: "The host address to bind the elasticsearch HTTP service to and to publish for HTTP clients to connect to"
    default: 0.0.0.0
----
+
. Create a subdirectory called `config` in the job's template directory, and create a new template called `config.yml.erb`
+
----
bootstrap.mlockall: true

path.conf: "/var/vcap/jobs/elasticsearch17/config"
path.logs: "/var/vcap/sys/log/elasticsearch17"
path.data: "/var/vcap/store/elasticsearch17"
path.scripts: "/var/vcap/data/elasticsearch17/plugin-scripts"

cluster.name: "<%= p("elasticsearch.cluster_name") %>"

node.max_local_storage_nodes: 1
node.name: "<%= name %>/<%= index %>"
node.master: <%= p("elasticsearch.node.allow_master") %>
node.data: <%= p("elasticsearch.node.allow_data") %>
node.job_name: "<%= name %>"
node.job_index: "<%= index %>"
<% p("elasticsearch.node.tags", {}).each do | k, v | %>
node.<%= k %>: "<%= v %>"
<% end %>

network.host: "0.0.0.0"
http.host: <%= p("elasticsearch.http_host") %>

discovery.zen.minimum_master_nodes: <%= p("elasticsearch.discovery.minimum_master_nodes") %>
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: "<%= p("elasticsearch.master_hosts").join(',') %>"

<% if_p('elasticsearch.config_options') do | v | %><%= v %><% end %>
----
+
. Create another template called `logging.yml.erb`
+
----
rootLogger: "<%= p("elasticsearch.log_level") %>, console"

appender:
  console:
    type: "console"
    layout:
      type: "consolePattern"
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

<% if_p('elasticsearch.logging_options') do | v | %><%= v %><% end %>
----
+ 
. In our install of elasticsearch, we want to enable customization of synonyms and dimensions.  To do this, add the custom/catalog subdirectory to the config directory, and copy your synonyms.txt and dimensions.txt files to it.
. Finally, there are a few common utility scripts that I'm leveraging which need to be created. These utility scripts set environment variables, paths, java_home, etc. From the templates directory, create a subdirectory called `data` and add a file called properties.sh.erb with this content:
+
----
#!/usr/bin/env bash

# job template binding variables

# job name & index of this VM within cluster
# e.g. JOB_NAME=redis, JOB_INDEX=0
export NAME='<%= name %>'
export JOB_INDEX=<%= index %>
# full job name, like redis/0 or webapp/3
export JOB_FULL="$NAME/$JOB_INDEX"
----
+ 
. And then, from the templates directory, create another subdirectory called `helpers` - it will have two files, the first called `ctl_setup.sh` with this content:
+
----
#!/usr/bin/env bash

# Setup env vars and folders for the ctl script
# This helps keep the ctl script as readable
# as possible

# Usage options:
# source /var/vcap/jobs/foobar/helpers/ctl_setup.sh JOB_NAME OUTPUT_LABEL
# source /var/vcap/jobs/foobar/helpers/ctl_setup.sh foobar
# source /var/vcap/jobs/foobar/helpers/ctl_setup.sh foobar foobar
# source /var/vcap/jobs/foobar/helpers/ctl_setup.sh foobar nginx

set -e # exit immediately if a simple command exits with a non-zero status
set -u # report the usage of uninitialized variables

JOB_NAME=$1
output_label=${1:-JOB_NAME}

export JOB_DIR=/var/vcap/jobs/$JOB_NAME
chmod 755 $JOB_DIR # to access file via symlink

# Load some bosh deployment properties into env vars
# Try to put all ERb into data/properties.sh.erb
# incl $NAME, $JOB_INDEX, $WEBAPP_DIR
source $JOB_DIR/data/properties.sh

source $JOB_DIR/helpers/ctl_utils.sh
redirect_output ${output_label}

export HOME=${HOME:-/home/vcap}

# Add all packages' /bin & /sbin into $PATH
for package_bin_dir in $(ls -d /var/vcap/packages/*/*bin)
do
  export PATH=${package_bin_dir}:$PATH
done

export LD_LIBRARY_PATH=${LD_LIBRARY_PATH:-''} # default to empty
for package_bin_dir in $(ls -d /var/vcap/packages/*/lib)
do
  export LD_LIBRARY_PATH=${package_bin_dir}:$LD_LIBRARY_PATH
done

# Setup log, run and tmp folders

export RUN_DIR=/var/vcap/sys/run/$JOB_NAME
export LOG_DIR=/var/vcap/sys/log/$JOB_NAME
export TMP_DIR=/var/vcap/sys/tmp/$JOB_NAME
export STORE_DIR=/var/vcap/store/$JOB_NAME
for dir in $RUN_DIR $LOG_DIR $TMP_DIR $STORE_DIR
do
  mkdir -p ${dir}
  chown vcap:vcap ${dir}
  chmod 775 ${dir}
done
export TMPDIR=$TMP_DIR

export C_INCLUDE_PATH=/var/vcap/packages/mysqlclient/include/mysql:/var/vcap/packages/sqlite/include:/var/vcap/packages/libpq/include
export LIBRARY_PATH=/var/vcap/packages/mysqlclient/lib/mysql:/var/vcap/packages/sqlite/lib:/var/vcap/packages/libpq/lib

# consistent place for vendoring python libraries within package
if [[ -d ${WEBAPP_DIR:-/xxxx} ]]
then
  export PYTHONPATH=$WEBAPP_DIR/vendor/lib/python
fi

if [[ -d /var/vcap/packages/java8 ]]
then
  export JAVA_HOME="/var/vcap/packages/java8"
fi

export PIDFILE=$RUN_DIR/$JOB_NAME.pid

echo '$PATH' $PATH
----
+
. And the second called `ctl_utils.sh` with this content
+
----
# Helper functions used by ctl scripts

# links a job file (probably a config file) into a package
# Example usage:
# link_job_file_to_package config/redis.yml [config/redis.yml]
# link_job_file_to_package config/wp-config.php wp-config.php
link_job_file_to_package() {
  source_job_file=$1
  target_package_file=${2:-$source_job_file}
  full_package_file=$WEBAPP_DIR/${target_package_file}

  link_job_file ${source_job_file} ${full_package_file}
}

# links a job file (probably a config file) somewhere
# Example usage:
# link_job_file config/bashrc /home/vcap/.bashrc
link_job_file() {
  source_job_file=$1
  target_file=$2
  full_job_file=$JOB_DIR/${source_job_file}

  echo link_job_file ${full_job_file} ${target_file}
  if [[ ! -f ${full_job_file} ]]
  then
    echo "file to link ${full_job_file} does not exist"
  else
    # Create/recreate the symlink to current job file
    # If another process is using the file, it won't be
    # deleted, so don't attempt to create the symlink
    mkdir -p $(dirname ${target_file})
    ln -nfs ${full_job_file} ${target_file}
  fi
}

# If loaded within monit ctl scripts then pipe output
# If loaded from 'source ../utils.sh' then normal STDOUT
redirect_output() {
  SCRIPT=$1
  mkdir -p /var/vcap/sys/log/monit
  exec 1>> /var/vcap/sys/log/monit/$SCRIPT.log
  exec 2>> /var/vcap/sys/log/monit/$SCRIPT.err.log
}

function pid_is_running() {
  declare pid="$1"
  ps -p "${pid}" >/dev/null 2>&1
}

# pid_guard
#
# @param pidfile
# @param name [String] an arbitrary name that might show up in STDOUT on errors
#
# Run this before attempting to start new processes that may use the same :pidfile:.
# If an old process is running on the pid found in the :pidfile:, exit 1. Otherwise,
# remove the stale :pidfile: if it exists.
#
function pid_guard() {
  declare pidfile="$1" name="$2"

  echo "------------ STARTING $(basename "$0") at $(date) --------------" | tee /dev/stderr

  if [ ! -f "${pidfile}" ]; then
    return 0
  fi

  local pid
  pid=$(head -1 "${pidfile}")

  if pid_is_running "${pid}"; then
    echo "${name} is already running, please stop it first"
    exit 1
  fi

  echo "Removing stale pidfile"
  rm "${pidfile}"
}

# wait_pid_death
#
# @param pid
# @param timeout
#
# Watch a :pid: for :timeout: seconds, waiting for it to die.
# If it dies before :timeout:, exit 0. If not, exit 1.
#
# Note that this should be run in a subshell, so that the current
# shell does not exit.
#
function wait_pid_death() {
  declare pid="$1" timeout="$2"

  local countdown
  countdown=$(( timeout * 10 ))

  while true; do
    if ! pid_is_running "${pid}"; then
      return 0
    fi

    if [ ${countdown} -le 0 ]; then
      return 1
    fi

    countdown=$(( countdown - 1 ))
    sleep 0.1
  done
}

# kill_and_wait
#
# @param pidfile
# @param timeout [default 25s]
#
# For a pid found in :pidfile:, send a `kill -15` TERM, then wait for :timeout: seconds to
# see if it dies on its own. If not, send it a `kill -9`. If the process does die,
# exit 0 and remove the :pidfile:. If after all of this, the process does not actually
# die, exit 1.
#
# Note:
# Monit default timeout for start/stop is 30s
# Append 'with timeout {n} seconds' to monit start/stop program configs
#
function kill_and_wait() {
  declare pidfile="$1" timeout="${2:-25}" sigkill_on_timeout="${3:-1}"

  if [ ! -f "${pidfile}" ]; then
    echo "Pidfile ${pidfile} doesn't exist"
    exit 0
  fi

  local pid
  pid=$(head -1 "${pidfile}")

  if [ -z "${pid}" ]; then
    echo "Unable to get pid from ${pidfile}"
    exit 1
  fi

  if ! pid_is_running "${pid}"; then
    echo "Process ${pid} is not running"
    rm -f "${pidfile}"
    exit 0
  fi

  echo "Killing ${pidfile}: ${pid} "
  kill "${pid}"

  if ! wait_pid_death "${pid}" "${timeout}"; then
    if [ "${sigkill_on_timeout}" = "1" ]; then
      echo "Kill timed out, using kill -9 on ${pid}"
      kill -9 "${pid}"
      sleep 0.5
    fi
  fi

  if pid_is_running "${pid}"; then
    echo "Timed Out"
    exit 1
  else
    echo "Stopped"
    rm -f "${pidfile}"
  fi
}

check_nfs_mount() {
  opts=$1
  exports=$2
  mount_point=$3

  if grep -qs $mount_point /proc/mounts; then
    echo "Found NFS mount $mount_point"
  else
    echo "Mounting NFS..."
    mount $opts $exports $mount_point
    if [ $? != 0 ]; then
      echo "Cannot mount NFS from $exports to $mount_point, exiting..."
      exit 1
    fi
  fi
}
----
+
. OK.  We're done with the release.  Let's get it packaged and ready to go.
+
----
> bosh create release --force
----
+
. At this point, we would upload this release to our bosh director.  The lab will end here, and I will demonstrate what this looks like on your environment.

